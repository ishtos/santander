{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import os\n",
    "import gc \n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb \n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from scipy.spatial import distance\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "\n",
    "NFOLDS = 5\n",
    "SEED = 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.insert(1, 'target', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7   ...     var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.6266   ...      4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
       "1  16.5338   ...      7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
       "2  14.6155   ...      2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n",
       "3  14.9250   ...      4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n",
       "4  19.2514   ...     -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   8.1267   8.7889  18.3560   1.9518  \n",
       "2  -6.5213   8.2675  14.7222   0.3965  \n",
       "3  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0656</td>\n",
       "      <td>7.7798</td>\n",
       "      <td>12.9536</td>\n",
       "      <td>9.4292</td>\n",
       "      <td>11.4327</td>\n",
       "      <td>-2.3805</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>18.2675</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.1556</td>\n",
       "      <td>11.8495</td>\n",
       "      <td>-1.4300</td>\n",
       "      <td>2.4508</td>\n",
       "      <td>13.7112</td>\n",
       "      <td>2.4669</td>\n",
       "      <td>4.3654</td>\n",
       "      <td>10.7200</td>\n",
       "      <td>15.4722</td>\n",
       "      <td>-8.7197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.5304</td>\n",
       "      <td>1.2543</td>\n",
       "      <td>11.3047</td>\n",
       "      <td>5.1858</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>-4.0117</td>\n",
       "      <td>6.0196</td>\n",
       "      <td>18.6316</td>\n",
       "      <td>...</td>\n",
       "      <td>10.6165</td>\n",
       "      <td>8.8349</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>10.1282</td>\n",
       "      <td>15.5765</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>-1.4852</td>\n",
       "      <td>9.8714</td>\n",
       "      <td>19.1293</td>\n",
       "      <td>-20.9760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.4827</td>\n",
       "      <td>-10.3581</td>\n",
       "      <td>10.1407</td>\n",
       "      <td>7.0479</td>\n",
       "      <td>10.2628</td>\n",
       "      <td>9.8052</td>\n",
       "      <td>4.8950</td>\n",
       "      <td>20.2537</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7484</td>\n",
       "      <td>10.9935</td>\n",
       "      <td>1.9803</td>\n",
       "      <td>2.1800</td>\n",
       "      <td>12.9813</td>\n",
       "      <td>2.1281</td>\n",
       "      <td>-7.1086</td>\n",
       "      <td>7.0618</td>\n",
       "      <td>19.8956</td>\n",
       "      <td>-23.1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0</td>\n",
       "      <td>8.5374</td>\n",
       "      <td>-1.3222</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>6.5749</td>\n",
       "      <td>8.8458</td>\n",
       "      <td>3.1744</td>\n",
       "      <td>4.9397</td>\n",
       "      <td>20.5660</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5702</td>\n",
       "      <td>9.0766</td>\n",
       "      <td>1.6580</td>\n",
       "      <td>3.5813</td>\n",
       "      <td>15.1874</td>\n",
       "      <td>3.1656</td>\n",
       "      <td>3.9567</td>\n",
       "      <td>9.2295</td>\n",
       "      <td>13.0168</td>\n",
       "      <td>-4.2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0</td>\n",
       "      <td>11.7058</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>7.7506</td>\n",
       "      <td>9.1035</td>\n",
       "      <td>-8.5848</td>\n",
       "      <td>6.8595</td>\n",
       "      <td>10.6048</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2259</td>\n",
       "      <td>9.1723</td>\n",
       "      <td>1.2835</td>\n",
       "      <td>3.3778</td>\n",
       "      <td>19.5542</td>\n",
       "      <td>-0.2860</td>\n",
       "      <td>-5.1612</td>\n",
       "      <td>7.2882</td>\n",
       "      <td>13.9260</td>\n",
       "      <td>-9.1846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code  target    var_0    var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  test_0       0  11.0656   7.7798  12.9536  9.4292  11.4327 -2.3805  5.8493   \n",
       "1  test_1       0   8.5304   1.2543  11.3047  5.1858   9.1974 -4.0117  6.0196   \n",
       "2  test_2       0   5.4827 -10.3581  10.1407  7.0479  10.2628  9.8052  4.8950   \n",
       "3  test_3       0   8.5374  -1.3222  12.0220  6.5749   8.8458  3.1744  4.9397   \n",
       "4  test_4       0  11.7058  -0.1327  14.1295  7.7506   9.1035 -8.5848  6.8595   \n",
       "\n",
       "     var_7   ...     var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.2675   ...     -2.1556  11.8495  -1.4300   2.4508  13.7112   2.4669   \n",
       "1  18.6316   ...     10.6165   8.8349   0.9403  10.1282  15.5765   0.4773   \n",
       "2  20.2537   ...     -0.7484  10.9935   1.9803   2.1800  12.9813   2.1281   \n",
       "3  20.5660   ...      9.5702   9.0766   1.6580   3.5813  15.1874   3.1656   \n",
       "4  10.6048   ...      4.2259   9.1723   1.2835   3.3778  19.5542  -0.2860   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   4.3654  10.7200  15.4722  -8.7197  \n",
       "1  -1.4852   9.8714  19.1293 -20.9760  \n",
       "2  -7.1086   7.0618  19.8956 -23.1794  \n",
       "3   3.9567   9.2295  13.0168  -4.2108  \n",
       "4  -5.1612   7.2882  13.9260  -9.1846  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = train.index\n",
    "test_index = test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_columns = [f'var_{i}' for i in range(0, 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df[var_columns])\n",
    "df[var_columns] = scaler.transform(df[var_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394711</td>\n",
       "      <td>0.324824</td>\n",
       "      <td>0.568059</td>\n",
       "      <td>0.388041</td>\n",
       "      <td>0.550670</td>\n",
       "      <td>0.467301</td>\n",
       "      <td>0.465762</td>\n",
       "      <td>0.578688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537807</td>\n",
       "      <td>0.342943</td>\n",
       "      <td>0.568958</td>\n",
       "      <td>0.448173</td>\n",
       "      <td>0.510471</td>\n",
       "      <td>0.291973</td>\n",
       "      <td>0.678981</td>\n",
       "      <td>0.412164</td>\n",
       "      <td>0.320222</td>\n",
       "      <td>0.564556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.511048</td>\n",
       "      <td>0.428639</td>\n",
       "      <td>0.681235</td>\n",
       "      <td>0.410417</td>\n",
       "      <td>0.628408</td>\n",
       "      <td>0.795039</td>\n",
       "      <td>0.546339</td>\n",
       "      <td>0.487471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630884</td>\n",
       "      <td>0.536531</td>\n",
       "      <td>0.523717</td>\n",
       "      <td>0.756190</td>\n",
       "      <td>0.349865</td>\n",
       "      <td>0.743892</td>\n",
       "      <td>0.686614</td>\n",
       "      <td>0.447856</td>\n",
       "      <td>0.595713</td>\n",
       "      <td>0.609337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380425</td>\n",
       "      <td>0.483777</td>\n",
       "      <td>0.578061</td>\n",
       "      <td>0.599690</td>\n",
       "      <td>0.474941</td>\n",
       "      <td>0.471310</td>\n",
       "      <td>0.758477</td>\n",
       "      <td>0.403860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493407</td>\n",
       "      <td>0.643141</td>\n",
       "      <td>0.448960</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.670521</td>\n",
       "      <td>0.856859</td>\n",
       "      <td>0.236337</td>\n",
       "      <td>0.365293</td>\n",
       "      <td>0.416170</td>\n",
       "      <td>0.586451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.491161</td>\n",
       "      <td>0.507140</td>\n",
       "      <td>0.396562</td>\n",
       "      <td>0.546993</td>\n",
       "      <td>0.647586</td>\n",
       "      <td>0.616796</td>\n",
       "      <td>0.581965</td>\n",
       "      <td>0.417350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538713</td>\n",
       "      <td>0.383085</td>\n",
       "      <td>0.370986</td>\n",
       "      <td>0.439205</td>\n",
       "      <td>0.744819</td>\n",
       "      <td>0.406918</td>\n",
       "      <td>0.346810</td>\n",
       "      <td>0.685901</td>\n",
       "      <td>0.576626</td>\n",
       "      <td>0.448188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435886</td>\n",
       "      <td>0.533434</td>\n",
       "      <td>0.624133</td>\n",
       "      <td>0.504796</td>\n",
       "      <td>0.621079</td>\n",
       "      <td>0.702806</td>\n",
       "      <td>0.597644</td>\n",
       "      <td>0.605921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365804</td>\n",
       "      <td>0.629275</td>\n",
       "      <td>0.299888</td>\n",
       "      <td>0.697737</td>\n",
       "      <td>0.238566</td>\n",
       "      <td>0.382291</td>\n",
       "      <td>0.557507</td>\n",
       "      <td>0.560948</td>\n",
       "      <td>0.577995</td>\n",
       "      <td>0.450972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target     var_0     var_1     var_2     var_3     var_4  \\\n",
       "0  train_0       0  0.394711  0.324824  0.568059  0.388041  0.550670   \n",
       "1  train_1       0  0.511048  0.428639  0.681235  0.410417  0.628408   \n",
       "2  train_2       0  0.380425  0.483777  0.578061  0.599690  0.474941   \n",
       "3  train_3       0  0.491161  0.507140  0.396562  0.546993  0.647586   \n",
       "4  train_4       0  0.435886  0.533434  0.624133  0.504796  0.621079   \n",
       "\n",
       "      var_5     var_6     var_7    ...      var_190   var_191   var_192  \\\n",
       "0  0.467301  0.465762  0.578688    ...     0.537807  0.342943  0.568958   \n",
       "1  0.795039  0.546339  0.487471    ...     0.630884  0.536531  0.523717   \n",
       "2  0.471310  0.758477  0.403860    ...     0.493407  0.643141  0.448960   \n",
       "3  0.616796  0.581965  0.417350    ...     0.538713  0.383085  0.370986   \n",
       "4  0.702806  0.597644  0.605921    ...     0.365804  0.629275  0.299888   \n",
       "\n",
       "    var_193   var_194   var_195   var_196   var_197   var_198   var_199  \n",
       "0  0.448173  0.510471  0.291973  0.678981  0.412164  0.320222  0.564556  \n",
       "1  0.756190  0.349865  0.743892  0.686614  0.447856  0.595713  0.609337  \n",
       "2  0.448000  0.670521  0.856859  0.236337  0.365293  0.416170  0.586451  \n",
       "3  0.439205  0.744819  0.406918  0.346810  0.685901  0.576626  0.448188  \n",
       "4  0.697737  0.238566  0.382291  0.557507  0.560948  0.577995  0.450972  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.iloc[train_index]\n",
    "test = df.iloc[test_index]\n",
    "\n",
    "y = train['target']\n",
    "\n",
    "not_use_cols = ['ID_code', 'target'] \n",
    "use_cols = [c for c in train.columns if c not in not_use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394711</td>\n",
       "      <td>0.324824</td>\n",
       "      <td>0.568059</td>\n",
       "      <td>0.388041</td>\n",
       "      <td>0.550670</td>\n",
       "      <td>0.467301</td>\n",
       "      <td>0.465762</td>\n",
       "      <td>0.578688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537807</td>\n",
       "      <td>0.342943</td>\n",
       "      <td>0.568958</td>\n",
       "      <td>0.448173</td>\n",
       "      <td>0.510471</td>\n",
       "      <td>0.291973</td>\n",
       "      <td>0.678981</td>\n",
       "      <td>0.412164</td>\n",
       "      <td>0.320222</td>\n",
       "      <td>0.564556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.511048</td>\n",
       "      <td>0.428639</td>\n",
       "      <td>0.681235</td>\n",
       "      <td>0.410417</td>\n",
       "      <td>0.628408</td>\n",
       "      <td>0.795039</td>\n",
       "      <td>0.546339</td>\n",
       "      <td>0.487471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630884</td>\n",
       "      <td>0.536531</td>\n",
       "      <td>0.523717</td>\n",
       "      <td>0.756190</td>\n",
       "      <td>0.349865</td>\n",
       "      <td>0.743892</td>\n",
       "      <td>0.686614</td>\n",
       "      <td>0.447856</td>\n",
       "      <td>0.595713</td>\n",
       "      <td>0.609337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380425</td>\n",
       "      <td>0.483777</td>\n",
       "      <td>0.578061</td>\n",
       "      <td>0.599690</td>\n",
       "      <td>0.474941</td>\n",
       "      <td>0.471310</td>\n",
       "      <td>0.758477</td>\n",
       "      <td>0.403860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493407</td>\n",
       "      <td>0.643141</td>\n",
       "      <td>0.448960</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.670521</td>\n",
       "      <td>0.856859</td>\n",
       "      <td>0.236337</td>\n",
       "      <td>0.365293</td>\n",
       "      <td>0.416170</td>\n",
       "      <td>0.586451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.491161</td>\n",
       "      <td>0.507140</td>\n",
       "      <td>0.396562</td>\n",
       "      <td>0.546993</td>\n",
       "      <td>0.647586</td>\n",
       "      <td>0.616796</td>\n",
       "      <td>0.581965</td>\n",
       "      <td>0.417350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538713</td>\n",
       "      <td>0.383085</td>\n",
       "      <td>0.370986</td>\n",
       "      <td>0.439205</td>\n",
       "      <td>0.744819</td>\n",
       "      <td>0.406918</td>\n",
       "      <td>0.346810</td>\n",
       "      <td>0.685901</td>\n",
       "      <td>0.576626</td>\n",
       "      <td>0.448188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435886</td>\n",
       "      <td>0.533434</td>\n",
       "      <td>0.624133</td>\n",
       "      <td>0.504796</td>\n",
       "      <td>0.621079</td>\n",
       "      <td>0.702806</td>\n",
       "      <td>0.597644</td>\n",
       "      <td>0.605921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365804</td>\n",
       "      <td>0.629275</td>\n",
       "      <td>0.299888</td>\n",
       "      <td>0.697737</td>\n",
       "      <td>0.238566</td>\n",
       "      <td>0.382291</td>\n",
       "      <td>0.557507</td>\n",
       "      <td>0.560948</td>\n",
       "      <td>0.577995</td>\n",
       "      <td>0.450972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target     var_0     var_1     var_2     var_3     var_4  \\\n",
       "0  train_0       0  0.394711  0.324824  0.568059  0.388041  0.550670   \n",
       "1  train_1       0  0.511048  0.428639  0.681235  0.410417  0.628408   \n",
       "2  train_2       0  0.380425  0.483777  0.578061  0.599690  0.474941   \n",
       "3  train_3       0  0.491161  0.507140  0.396562  0.546993  0.647586   \n",
       "4  train_4       0  0.435886  0.533434  0.624133  0.504796  0.621079   \n",
       "\n",
       "      var_5     var_6     var_7    ...      var_190   var_191   var_192  \\\n",
       "0  0.467301  0.465762  0.578688    ...     0.537807  0.342943  0.568958   \n",
       "1  0.795039  0.546339  0.487471    ...     0.630884  0.536531  0.523717   \n",
       "2  0.471310  0.758477  0.403860    ...     0.493407  0.643141  0.448960   \n",
       "3  0.616796  0.581965  0.417350    ...     0.538713  0.383085  0.370986   \n",
       "4  0.702806  0.597644  0.605921    ...     0.365804  0.629275  0.299888   \n",
       "\n",
       "    var_193   var_194   var_195   var_196   var_197   var_198   var_199  \n",
       "0  0.448173  0.510471  0.291973  0.678981  0.412164  0.320222  0.564556  \n",
       "1  0.756190  0.349865  0.743892  0.686614  0.447856  0.595713  0.609337  \n",
       "2  0.448000  0.670521  0.856859  0.236337  0.365293  0.416170  0.586451  \n",
       "3  0.439205  0.744819  0.406918  0.346810  0.685901  0.576626  0.448188  \n",
       "4  0.697737  0.238566  0.382291  0.557507  0.560948  0.577995  0.450972  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394711</td>\n",
       "      <td>0.324824</td>\n",
       "      <td>0.568059</td>\n",
       "      <td>0.388041</td>\n",
       "      <td>0.550670</td>\n",
       "      <td>0.467301</td>\n",
       "      <td>0.465762</td>\n",
       "      <td>0.578688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537807</td>\n",
       "      <td>0.342943</td>\n",
       "      <td>0.568958</td>\n",
       "      <td>0.448173</td>\n",
       "      <td>0.510471</td>\n",
       "      <td>0.291973</td>\n",
       "      <td>0.678981</td>\n",
       "      <td>0.412164</td>\n",
       "      <td>0.320222</td>\n",
       "      <td>0.564556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.511048</td>\n",
       "      <td>0.428639</td>\n",
       "      <td>0.681235</td>\n",
       "      <td>0.410417</td>\n",
       "      <td>0.628408</td>\n",
       "      <td>0.795039</td>\n",
       "      <td>0.546339</td>\n",
       "      <td>0.487471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630884</td>\n",
       "      <td>0.536531</td>\n",
       "      <td>0.523717</td>\n",
       "      <td>0.756190</td>\n",
       "      <td>0.349865</td>\n",
       "      <td>0.743892</td>\n",
       "      <td>0.686614</td>\n",
       "      <td>0.447856</td>\n",
       "      <td>0.595713</td>\n",
       "      <td>0.609337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380425</td>\n",
       "      <td>0.483777</td>\n",
       "      <td>0.578061</td>\n",
       "      <td>0.599690</td>\n",
       "      <td>0.474941</td>\n",
       "      <td>0.471310</td>\n",
       "      <td>0.758477</td>\n",
       "      <td>0.403860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493407</td>\n",
       "      <td>0.643141</td>\n",
       "      <td>0.448960</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.670521</td>\n",
       "      <td>0.856859</td>\n",
       "      <td>0.236337</td>\n",
       "      <td>0.365293</td>\n",
       "      <td>0.416170</td>\n",
       "      <td>0.586451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.491161</td>\n",
       "      <td>0.507140</td>\n",
       "      <td>0.396562</td>\n",
       "      <td>0.546993</td>\n",
       "      <td>0.647586</td>\n",
       "      <td>0.616796</td>\n",
       "      <td>0.581965</td>\n",
       "      <td>0.417350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538713</td>\n",
       "      <td>0.383085</td>\n",
       "      <td>0.370986</td>\n",
       "      <td>0.439205</td>\n",
       "      <td>0.744819</td>\n",
       "      <td>0.406918</td>\n",
       "      <td>0.346810</td>\n",
       "      <td>0.685901</td>\n",
       "      <td>0.576626</td>\n",
       "      <td>0.448188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435886</td>\n",
       "      <td>0.533434</td>\n",
       "      <td>0.624133</td>\n",
       "      <td>0.504796</td>\n",
       "      <td>0.621079</td>\n",
       "      <td>0.702806</td>\n",
       "      <td>0.597644</td>\n",
       "      <td>0.605921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365804</td>\n",
       "      <td>0.629275</td>\n",
       "      <td>0.299888</td>\n",
       "      <td>0.697737</td>\n",
       "      <td>0.238566</td>\n",
       "      <td>0.382291</td>\n",
       "      <td>0.557507</td>\n",
       "      <td>0.560948</td>\n",
       "      <td>0.577995</td>\n",
       "      <td>0.450972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target     var_0     var_1     var_2     var_3     var_4  \\\n",
       "0  train_0       0  0.394711  0.324824  0.568059  0.388041  0.550670   \n",
       "1  train_1       0  0.511048  0.428639  0.681235  0.410417  0.628408   \n",
       "2  train_2       0  0.380425  0.483777  0.578061  0.599690  0.474941   \n",
       "3  train_3       0  0.491161  0.507140  0.396562  0.546993  0.647586   \n",
       "4  train_4       0  0.435886  0.533434  0.624133  0.504796  0.621079   \n",
       "\n",
       "      var_5     var_6     var_7    ...      var_190   var_191   var_192  \\\n",
       "0  0.467301  0.465762  0.578688    ...     0.537807  0.342943  0.568958   \n",
       "1  0.795039  0.546339  0.487471    ...     0.630884  0.536531  0.523717   \n",
       "2  0.471310  0.758477  0.403860    ...     0.493407  0.643141  0.448960   \n",
       "3  0.616796  0.581965  0.417350    ...     0.538713  0.383085  0.370986   \n",
       "4  0.702806  0.597644  0.605921    ...     0.365804  0.629275  0.299888   \n",
       "\n",
       "    var_193   var_194   var_195   var_196   var_197   var_198   var_199  \n",
       "0  0.448173  0.510471  0.291973  0.678981  0.412164  0.320222  0.564556  \n",
       "1  0.756190  0.349865  0.743892  0.686614  0.447856  0.595713  0.609337  \n",
       "2  0.448000  0.670521  0.856859  0.236337  0.365293  0.416170  0.586451  \n",
       "3  0.439205  0.744819  0.406918  0.346810  0.685901  0.576626  0.448188  \n",
       "4  0.697737  0.238566  0.382291  0.557507  0.560948  0.577995  0.450972  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[use_cols]\n",
    "X_test = test[use_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pseudo labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_lightgbm(X, y, X_test, NFOLDS=5, SEED=6):\n",
    "    params_in_train = {\n",
    "        'num_boost_round': 20000,\n",
    "        'early_stopping_rounds': 200,\n",
    "        'verbose_eval': 500,\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    scores = {'train': [], 'valid': []}\n",
    "    features = X.columns\n",
    "    feature_importance_df = pd.DataFrame(columns=['fold', 'feature', 'importance'])\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
    "        print('fold: {}/{}'.format(fold+1, skf.n_splits))\n",
    "        \n",
    "        params = {\n",
    "            'boosting': 'gbdt',\n",
    "            'metric': 'auc',\n",
    "            'objective': 'binary',\n",
    "            'max_depth': 6,\n",
    "            'num_leaves': 12,\n",
    "            'min_data_in_leaf': 64,\n",
    "            'bagging_freq': 5,\n",
    "            'learning_rate': 0.01,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.4,\n",
    "            'reg_alpha': 2,\n",
    "            'reg_lambda': 5,\n",
    "            'min_gain_to_split': 0.01,\n",
    "            'min_child_weight': 19,\n",
    "            'num_threads': cpu_count(),\n",
    "            'verbose': -1,\n",
    "            'seed': int(2**fold),\n",
    "            'bagging_seed': int(2**fold),\n",
    "            'drop_seed': int(2**fold),\n",
    "        }\n",
    "        \n",
    "        dtrain = lgb.Dataset(X.iloc[train_index], label=y.iloc[train_index])\n",
    "        dvalid = lgb.Dataset(X.iloc[valid_index], label=y.iloc[valid_index])\n",
    "        \n",
    "        model = lgb.train(params, dtrain, valid_sets=[dtrain, dvalid], **params_in_train)\n",
    "        scores['train'].append(model.best_score['training']['auc'])\n",
    "        scores['valid'].append(model.best_score['valid_1']['auc'])\n",
    "        oof[valid_index] = model.predict(X.iloc[valid_index], num_iteration=model.best_iteration)\n",
    "\n",
    "        fold_feature_importance_df = pd.DataFrame(columns=['fold', 'feature', 'importance'])\n",
    "        fold_feature_importance_df['feature'] = features\n",
    "        fold_feature_importance_df['importance'] = model.feature_importance()\n",
    "        fold_feature_importance_df['fold'] = fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_feature_importance_df], axis=0)\n",
    "\n",
    "        predictions += model.predict(X_test, num_iteration=model.best_iteration) / NFOLDS\n",
    "\n",
    "        del model\n",
    "\n",
    "    return oof, predictions, scores, feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1/5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's auc: 0.856901\tvalid_1's auc: 0.83986\n",
      "[1000]\ttraining's auc: 0.888501\tvalid_1's auc: 0.868334\n",
      "[1500]\ttraining's auc: 0.903668\tvalid_1's auc: 0.881191\n",
      "[2000]\ttraining's auc: 0.91335\tvalid_1's auc: 0.888647\n",
      "[2500]\ttraining's auc: 0.919919\tvalid_1's auc: 0.893129\n",
      "[3000]\ttraining's auc: 0.92477\tvalid_1's auc: 0.896052\n",
      "[3500]\ttraining's auc: 0.928459\tvalid_1's auc: 0.898048\n",
      "[4000]\ttraining's auc: 0.931538\tvalid_1's auc: 0.899456\n",
      "[4500]\ttraining's auc: 0.934214\tvalid_1's auc: 0.900293\n",
      "[5000]\ttraining's auc: 0.936633\tvalid_1's auc: 0.900942\n",
      "[5500]\ttraining's auc: 0.939036\tvalid_1's auc: 0.901316\n",
      "[6000]\ttraining's auc: 0.941323\tvalid_1's auc: 0.901578\n",
      "Early stopping, best iteration is:\n",
      "[5915]\ttraining's auc: 0.940933\tvalid_1's auc: 0.901616\n",
      "fold: 2/5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's auc: 0.857364\tvalid_1's auc: 0.839918\n",
      "[1000]\ttraining's auc: 0.888712\tvalid_1's auc: 0.86768\n",
      "[1500]\ttraining's auc: 0.904373\tvalid_1's auc: 0.880121\n",
      "[2000]\ttraining's auc: 0.913657\tvalid_1's auc: 0.88677\n",
      "[2500]\ttraining's auc: 0.920207\tvalid_1's auc: 0.891213\n",
      "[3000]\ttraining's auc: 0.925037\tvalid_1's auc: 0.893956\n",
      "[3500]\ttraining's auc: 0.928883\tvalid_1's auc: 0.89615\n",
      "[4000]\ttraining's auc: 0.932078\tvalid_1's auc: 0.897212\n",
      "[4500]\ttraining's auc: 0.93474\tvalid_1's auc: 0.898039\n",
      "[5000]\ttraining's auc: 0.937151\tvalid_1's auc: 0.898677\n",
      "[5500]\ttraining's auc: 0.939506\tvalid_1's auc: 0.899062\n",
      "Early stopping, best iteration is:\n",
      "[5480]\ttraining's auc: 0.939404\tvalid_1's auc: 0.899077\n",
      "fold: 3/5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's auc: 0.859119\tvalid_1's auc: 0.836771\n",
      "[1000]\ttraining's auc: 0.890119\tvalid_1's auc: 0.863071\n",
      "[1500]\ttraining's auc: 0.905474\tvalid_1's auc: 0.875234\n",
      "[2000]\ttraining's auc: 0.914733\tvalid_1's auc: 0.882227\n",
      "[2500]\ttraining's auc: 0.921424\tvalid_1's auc: 0.887171\n",
      "[3000]\ttraining's auc: 0.926146\tvalid_1's auc: 0.890192\n",
      "[3500]\ttraining's auc: 0.929939\tvalid_1's auc: 0.892262\n",
      "[4000]\ttraining's auc: 0.932844\tvalid_1's auc: 0.893623\n",
      "[4500]\ttraining's auc: 0.935542\tvalid_1's auc: 0.894477\n",
      "[5000]\ttraining's auc: 0.937895\tvalid_1's auc: 0.895146\n",
      "[5500]\ttraining's auc: 0.940131\tvalid_1's auc: 0.895464\n",
      "[6000]\ttraining's auc: 0.942422\tvalid_1's auc: 0.89563\n",
      "Early stopping, best iteration is:\n",
      "[6170]\ttraining's auc: 0.943172\tvalid_1's auc: 0.895793\n",
      "fold: 4/5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's auc: 0.858467\tvalid_1's auc: 0.834766\n",
      "[1000]\ttraining's auc: 0.889203\tvalid_1's auc: 0.86274\n",
      "[1500]\ttraining's auc: 0.904817\tvalid_1's auc: 0.876681\n",
      "[2000]\ttraining's auc: 0.914129\tvalid_1's auc: 0.883595\n",
      "[2500]\ttraining's auc: 0.920662\tvalid_1's auc: 0.888574\n",
      "[3000]\ttraining's auc: 0.925278\tvalid_1's auc: 0.891509\n",
      "[3500]\ttraining's auc: 0.929144\tvalid_1's auc: 0.893634\n",
      "[4000]\ttraining's auc: 0.932183\tvalid_1's auc: 0.894887\n",
      "[4500]\ttraining's auc: 0.934838\tvalid_1's auc: 0.895748\n",
      "[5000]\ttraining's auc: 0.937311\tvalid_1's auc: 0.896218\n",
      "[5500]\ttraining's auc: 0.939547\tvalid_1's auc: 0.896743\n",
      "[6000]\ttraining's auc: 0.941825\tvalid_1's auc: 0.897068\n",
      "[6500]\ttraining's auc: 0.944186\tvalid_1's auc: 0.897165\n",
      "Early stopping, best iteration is:\n",
      "[6329]\ttraining's auc: 0.943407\tvalid_1's auc: 0.897214\n",
      "fold: 5/5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's auc: 0.858632\tvalid_1's auc: 0.841549\n",
      "[1000]\ttraining's auc: 0.889557\tvalid_1's auc: 0.866817\n",
      "[1500]\ttraining's auc: 0.905008\tvalid_1's auc: 0.878833\n",
      "[2000]\ttraining's auc: 0.914489\tvalid_1's auc: 0.885823\n",
      "[2500]\ttraining's auc: 0.92094\tvalid_1's auc: 0.889898\n",
      "[3000]\ttraining's auc: 0.925736\tvalid_1's auc: 0.892505\n",
      "[3500]\ttraining's auc: 0.92973\tvalid_1's auc: 0.894594\n",
      "[4000]\ttraining's auc: 0.932582\tvalid_1's auc: 0.895959\n",
      "[4500]\ttraining's auc: 0.935256\tvalid_1's auc: 0.896828\n",
      "[5000]\ttraining's auc: 0.937589\tvalid_1's auc: 0.897297\n",
      "[5500]\ttraining's auc: 0.939835\tvalid_1's auc: 0.89752\n",
      "[6000]\ttraining's auc: 0.942126\tvalid_1's auc: 0.897605\n",
      "Early stopping, best iteration is:\n",
      "[5964]\ttraining's auc: 0.941959\tvalid_1's auc: 0.897671\n"
     ]
    }
   ],
   "source": [
    "oof, predictions, scores, feature_importance_df = cv_lightgbm(X, y, X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num folds: 5\n",
      "Train Scores: mean 0.94177, max 0.94341, min 0.93940, std 0.00148\n",
      "Valid Scores: mean 0.89827, max 0.90162, min 0.89579, std 0.00197\n",
      "CV Score: 0.94776 \n"
     ]
    }
   ],
   "source": [
    "cv_score = roc_auc_score(y, oof)**0.5\n",
    "print('Num folds: {}'.format(NFOLDS))\n",
    "print('Train Scores: mean {:.5f}, max {:.5f}, min {:.5f}, std {:.5f}'.format(\n",
    "    np.mean(scores['train']), np.max(scores['train']), np.min(scores['train']), np.std(scores['train'])))\n",
    "print('Valid Scores: mean {:.5f}, max {:.5f}, min {:.5f}, std {:.5f}'.format(\n",
    "    np.mean(scores['valid']), np.max(scores['valid']), np.min(scores['valid']), np.std(scores['valid'])))\n",
    "print('CV Score: {:<8.5f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([np.where(x >= 0.5, 1, 0) for x in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_pseudo_labeling_lightgbm(X, y, X_test, y_test, NFOLDS=5, SEED=6):\n",
    "    params_in_train = {\n",
    "        'num_boost_round': 20000,\n",
    "        'early_stopping_rounds': 200,\n",
    "        'verbose_eval': 500,\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    scores = {'train': [], 'valid': []}\n",
    "    features = X.columns\n",
    "    feature_importance_df = pd.DataFrame(columns=['fold', 'feature', 'importance'])\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
    "        print('fold: {}/{}'.format(fold+1, skf.n_splits))\n",
    "        \n",
    "        params = {\n",
    "            'boosting': 'gbdt',\n",
    "            'metric': 'binary',\n",
    "            'objective': 'binary',\n",
    "            'max_depth': 6,\n",
    "            'num_leaves': 12,\n",
    "            'min_data_in_leaf': 64,\n",
    "            'bagging_freq': 5,\n",
    "            'learning_rate': 0.01,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.4,\n",
    "            'reg_alpha': 2,\n",
    "            'reg_lambda': 5,\n",
    "            'min_gain_to_split': 0.01,\n",
    "            'min_child_weight': 19,\n",
    "            'num_threads': cpu_count(),\n",
    "            'verbose': -1,\n",
    "            'seed': int(2**fold),\n",
    "            'bagging_seed': int(2**fold),\n",
    "            'drop_seed': int(2**fold),\n",
    "        }\n",
    "        \n",
    "        X_train = pd.concat([X.iloc[train_index], X_test], axis=0)\n",
    "        y_train = np.concatenate([y.iloc[train_index], y_test], axis=0)\n",
    "        \n",
    "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "        dvalid = lgb.Dataset(X.iloc[valid_index], label=y.iloc[valid_index])\n",
    "        \n",
    "        model = lgb.train(params, dtrain, valid_sets=[dtrain, dvalid], **params_in_train)\n",
    "        scores['train'].append(model.best_score['training']['binary_logloss'])\n",
    "        scores['valid'].append(model.best_score['valid_1']['binary_logloss'])\n",
    "        oof[valid_index] = model.predict(X.iloc[valid_index], num_iteration=model.best_iteration)\n",
    "\n",
    "        fold_feature_importance_df = pd.DataFrame(columns=['fold', 'feature', 'importance'])\n",
    "        fold_feature_importance_df['feature'] = features\n",
    "        fold_feature_importance_df['importance'] = model.feature_importance()\n",
    "        fold_feature_importance_df['fold'] = fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_feature_importance_df], axis=0)\n",
    "\n",
    "        predictions += model.predict(X_test, num_iteration=model.best_iteration) / NFOLDS\n",
    "\n",
    "        del model\n",
    "\n",
    "    return oof, predictions, scores, feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1/5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's binary_logloss: 0.199069\tvalid_1's binary_logloss: 0.270463\n",
      "[1000]\ttraining's binary_logloss: 0.173971\tvalid_1's binary_logloss: 0.247183\n",
      "[1500]\ttraining's binary_logloss: 0.158493\tvalid_1's binary_logloss: 0.233828\n",
      "[2000]\ttraining's binary_logloss: 0.147708\tvalid_1's binary_logloss: 0.225103\n",
      "[2500]\ttraining's binary_logloss: 0.139754\tvalid_1's binary_logloss: 0.219424\n",
      "[3000]\ttraining's binary_logloss: 0.133534\tvalid_1's binary_logloss: 0.21537\n",
      "[3500]\ttraining's binary_logloss: 0.128591\tvalid_1's binary_logloss: 0.212306\n",
      "[4000]\ttraining's binary_logloss: 0.124544\tvalid_1's binary_logloss: 0.21018\n",
      "[4500]\ttraining's binary_logloss: 0.121198\tvalid_1's binary_logloss: 0.208649\n",
      "[5000]\ttraining's binary_logloss: 0.118387\tvalid_1's binary_logloss: 0.207864\n",
      "[5500]\ttraining's binary_logloss: 0.115987\tvalid_1's binary_logloss: 0.207236\n",
      "[6000]\ttraining's binary_logloss: 0.113918\tvalid_1's binary_logloss: 0.206989\n",
      "Early stopping, best iteration is:\n",
      "[6177]\ttraining's binary_logloss: 0.113249\tvalid_1's binary_logloss: 0.206861\n",
      "fold: 2/5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's binary_logloss: 0.199037\tvalid_1's binary_logloss: 0.270054\n",
      "[1000]\ttraining's binary_logloss: 0.173988\tvalid_1's binary_logloss: 0.247032\n",
      "[1500]\ttraining's binary_logloss: 0.158471\tvalid_1's binary_logloss: 0.233756\n",
      "[2000]\ttraining's binary_logloss: 0.147708\tvalid_1's binary_logloss: 0.225342\n",
      "[2500]\ttraining's binary_logloss: 0.139725\tvalid_1's binary_logloss: 0.219722\n",
      "[3000]\ttraining's binary_logloss: 0.133496\tvalid_1's binary_logloss: 0.215576\n",
      "[3500]\ttraining's binary_logloss: 0.128524\tvalid_1's binary_logloss: 0.212817\n",
      "[4000]\ttraining's binary_logloss: 0.124475\tvalid_1's binary_logloss: 0.210791\n",
      "[4500]\ttraining's binary_logloss: 0.121127\tvalid_1's binary_logloss: 0.209402\n",
      "[5000]\ttraining's binary_logloss: 0.118289\tvalid_1's binary_logloss: 0.208565\n",
      "[5500]\ttraining's binary_logloss: 0.115879\tvalid_1's binary_logloss: 0.208212\n",
      "[6000]\ttraining's binary_logloss: 0.113789\tvalid_1's binary_logloss: 0.208099\n",
      "Early stopping, best iteration is:\n",
      "[5885]\ttraining's binary_logloss: 0.114247\tvalid_1's binary_logloss: 0.208045\n",
      "fold: 3/5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's binary_logloss: 0.198861\tvalid_1's binary_logloss: 0.271399\n",
      "[1000]\ttraining's binary_logloss: 0.173718\tvalid_1's binary_logloss: 0.249091\n",
      "[1500]\ttraining's binary_logloss: 0.158151\tvalid_1's binary_logloss: 0.236089\n",
      "[2000]\ttraining's binary_logloss: 0.14739\tvalid_1's binary_logloss: 0.227895\n",
      "[2500]\ttraining's binary_logloss: 0.139363\tvalid_1's binary_logloss: 0.222448\n",
      "[3000]\ttraining's binary_logloss: 0.133139\tvalid_1's binary_logloss: 0.218609\n",
      "[3500]\ttraining's binary_logloss: 0.128168\tvalid_1's binary_logloss: 0.215817\n",
      "[4000]\ttraining's binary_logloss: 0.124106\tvalid_1's binary_logloss: 0.214028\n",
      "[4500]\ttraining's binary_logloss: 0.120759\tvalid_1's binary_logloss: 0.212823\n"
     ]
    }
   ],
   "source": [
    "oof, psudeo_predictions, scores, feature_importance_df = cv_pseudo_labeling_lightgbm(X, y, X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = roc_auc_score(y, oof)**0.5\n",
    "print('Num folds: {}'.format(NFOLDS))\n",
    "print('Train Scores: mean {:.5f}, max {:.5f}, min {:.5f}, std {:.5f}'.format(\n",
    "    np.mean(scores['train']), np.max(scores['train']), np.min(scores['train']), np.std(scores['train'])))\n",
    "print('Valid Scores: mean {:.5f}, max {:.5f}, min {:.5f}, std {:.5f}'.format(\n",
    "    np.mean(scores['valid']), np.max(scores['valid']), np.min(scores['valid']), np.std(scores['valid'])))\n",
    "print('CV Score: {:<8.5f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join('..', 'data', 'sample_submission.csv'))\n",
    "submission['target'] = predictions\n",
    "submission.to_csv(os.path.join('..', 'submission', '{}_lightgbm.csv'.format(str(datetime.datetime.today().date()).replace('-', ''))), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df['importance'] = feature_importance_df['importance'].astype('int')\n",
    "ordered_feature = feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).index\n",
    "plt.figure(figsize=(12, 48))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance_df, order=ordered_feature)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(submission['target'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join('..', 'submission', '20190317_lightgbm.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(submission['target'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
